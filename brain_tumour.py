# -*- coding: utf-8 -*-
"""Brain Tumour.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10tOPVXMcaBejHRL-r5bZD3sNnp3l2VQQ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as  plt
import cv2
import tensorflow as tf
from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,Flatten,InputLayer,Input
from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ModelCheckpoint
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam,SGD
from tensorflow.keras.applications import ResNet152V2,DenseNet121,InceptionResNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.metrics import Accuracy,TruePositives,TrueNegatives,FalseNegatives,FalsePositives
from tensorflow.keras.regularizers import L2
from sklearn.metrics import confusion_matrix,classification_report,roc_curve,roc_auc_score

directory = '/content/Brain Tumour'

class_names = ['no','yes']

dataset = tf.keras.utils.image_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=class_names,
    color_mode='rgb',
    batch_size=None,
    image_size=(256, 256),
    shuffle=True,
    seed=42
)

"""##Data Preprocessing
##Splits

"""

def splits(dataset,Train_ratio,Val_ratio,Test_ratio):

    data_size = len(dataset)

    train_dataset = dataset.take(int(data_size*Train_ratio))

    val_split_set = dataset.skip(int(data_size*Train_ratio))

    val_dataset = val_split_set.take((int(data_size*Val_ratio)))

    test_dataset = val_split_set.skip(int(data_size*Test_ratio))
    return train_dataset,val_dataset,test_dataset

Train_ratio = 0.80
Val_ratio = .10
Test_ratio = 0.10
train_dataset,val_dataset,test_dataset  = splits(dataset,Train_ratio,Val_ratio,Test_ratio)

print("The length of the train_dataset is  : ",len(train_dataset))
print('The length of the val_dataset is  : ',len(val_dataset))
print('The length of the test_dataset is  : ',len(test_dataset))

plt.figure(figsize=(8,8))
for i,(image,label) in enumerate(train_dataset.take(9)):
    ax = plt.subplot(3,3,i+1)
    plt.imshow(image/255)
    plt.title(class_names[label])
    plt.axis('off')

"""##Dataset Prepartion"""

#resize and rescaling

def resize_rescaling(image,label):

    return tf.image.resize(image,(224,224))/255,label

train_dataset = (
             train_dataset
            .shuffle(reshuffle_each_iteration=True,buffer_size=8)
            .map(resize_rescaling)
            .batch(10)
            .prefetch(tf.data.AUTOTUNE)
)

val_dataset = (
             val_dataset
            .shuffle(reshuffle_each_iteration=True,buffer_size=8)
            .map(resize_rescaling)
            .batch(3)
            .prefetch(tf.data.AUTOTUNE)
)

test_dataset = (
             test_dataset
            .shuffle(reshuffle_each_iteration=True,buffer_size=8)
            .map(resize_rescaling)
            .batch(1)
            .prefetch(tf.data.AUTOTUNE)
)

"""##Create Model

"""

Lenet_model = tf.keras.Sequential([
    InputLayer(shape=(224,224,3)),

    Conv2D(filters=32, kernel_size=3,strides=2,padding='valid',activation='relu',kernel_regularizer=L2(0.01)),
    Dropout(0.2),
    BatchNormalization(),
    MaxPooling2D(pool_size=3,strides=2),

    Conv2D(filters=32,kernel_size=3,strides=2,padding='valid',activation='relu',kernel_regularizer=L2(0.01)),

    BatchNormalization(),
    MaxPooling2D(pool_size=3,strides=1),

    Conv2D(filters=32,kernel_size=3,strides=2,padding='valid',activation='relu',kernel_regularizer=L2(0.01)),

    BatchNormalization(),

    MaxPooling2D(pool_size=3,strides=1),
    Dropout(0.2),


    Flatten(),

    Dense(64,activation='relu'),
    BatchNormalization(),

    Dense(64,activation='relu'),
    BatchNormalization(),

    Dense(1,activation='sigmoid')
],name='Lenet_model')
Lenet_model.summary()

Lenet_model.compile(loss = BinaryCrossentropy(),
                   optimizer=Adam(learning_rate=0.001),
                   metrics = ['accuracy'])

checkpoint_path = "/kaggle/working/best_model.keras"
checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_path,  # Corrected parameter name and file extension
    monitor='val_loss',  # Metric to monitor
    save_best_only=True,  # Save only the best model
    mode='min',  # 'min' for loss, 'max' for accuracy
    verbose=1
)


early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=30,
    verbose=1,
    mode='auto',
    restore_best_weights=True
)

history = Lenet_model.fit(train_dataset,
                          validation_data=val_dataset,
                          epochs=30,
                          verbose=1,
                          callbacks = [checkpoint_callback,
                                      early_stopping ]

                         )

"""##Evaluate the model"""

def evaluate_model(model,dataset):

    loss = model.history.history['loss']
    val_loss = model.history.history['val_loss']
    accuracy = model.history.history['accuracy']
    val_accuracy = model.history.history['val_accuracy']

    plt.figure(figsize=(14,12))

    # Plot model loss
    plt.subplot(2,2,1)

    plt.plot(val_loss)
    plt.title("Model Loss")
    plt.xlabel("Epochs")
    plt.ylabel('Loss')
    plt.legend(['train_loss','val_loss'])
    plt.grid()

    # Plot model accuracy
    plt.subplot(2,2,2)

    plt.plot(accuracy)
    plt.plot(val_accuracy)
    plt.title("Model Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel('Accuracy')
    plt.legend(['train_accuracy','val_accuracy'])
    plt.grid()

    # confusion matrix
    plt.subplot(2,2,3)

    img = []
    labels = []
    for x,y in dataset:
        img.append(x)
        labels.append(y)
    true_label = np.array(labels).flatten()
    predicted = model.predict(np.array(img)[:,0,...])[:,0]

    threshold = 0.5

    cm = confusion_matrix(true_label, predicted > threshold)



    sns.heatmap(cm, annot=True,cmap='Blues',xticklabels=['NO','YES'],yticklabels=['NO','YES'])
    plt.title('Confusion matrix - {}'.format(threshold))
    plt.ylabel('Actual')
    plt.xlabel('Predicted')

    # Plot roc curve
    plt.subplot(2,2,4)

    fp, tp, thresholds = roc_curve(true_label, predicted)
    roc_auc = roc_auc_score(true_label,predicted)
    plt.plot(fp,tp, color='blue',  label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0,1],[0,1],linestyle='--')
    plt.xlabel("False Positive rate")
    plt.ylabel("True Positive rate")

    plt.grid()

    skip = 2

    for i in range(0, len(thresholds), skip):
      plt.text(fp[i], tp[i], thresholds[i])
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc='lower right')
    plt.grid()
    plt.show()

evaluate_model(Lenet_model,test_dataset)

test_evaluate = Lenet_model.evaluate(test_dataset)
test_evaluate

val_evaluate = Lenet_model.evaluate(val_dataset)
val_evaluate

"""##Transform Learning

"""

resnet_backbone = ResNet152V2(
    include_top=False,
    weights='imagenet',
    input_shape= (224,224,3)
)
inputs = Input(shape=(224,224,3))
x = resnet_backbone(inputs)
x = Flatten()(x)
x = Dense(64,activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(64,activation='relu')(x)
x = BatchNormalization()(x)
outputs = Dense(1,activation='sigmoid')(x)
Resnet_model = Model(inputs,outputs,name='Resnet_model')
Resnet_model.compile(loss = BinaryCrossentropy(),
                   optimizer=Adam(learning_rate=0.0001),
                   metrics = ['accuracy'])
Resnet_model.fit(train_dataset,
                          validation_data=val_dataset,
                          epochs=2,
                          verbose=1

                         )

